## Annotation Details

We invite 20 participants to assess system responses from the sampled dialogues resorting to ten evaluation metrics we proposed at the dialogue exchange level.  We design a questionnaire containing 13 questions covering all evaluation perspectives, which refers to *recommendation rate*, *explanation rate*, *transparency*, *scrutability*, *reasonability*, *coherence*, *trust*, *persuasiveness*, *efficiency*, *satisfaction*, *representativeness*, *effectiveness* and *overall*. Since *representativeness*, *effectiveness*, and *overall* need the information to check, the annotators are asked to answer the first 10 questions only based on conversation and answer the last 3 questions with additional information of recommended movies, e.g., the introduction, the directors, the actors, and the reviews. Two different annotators annotate each response, and if there is a disagreement between them, the third annotator will involve. 



## **Questionnaire**

### Exchange level

1. **Does system recommend a new movie?**
   1. Yes
   2. No

2. **Does system provide an explanation?**
   1. Yes
   2. No
3. **After reading the explanation, do you know why system recommends these movies to you?**
   1. Yes
   2. No

4. **After reading the explanation, do you know what preferences the recommendation based on?**
   1. 1 point: I don't know.
   2. 2 point: I need to determine by contextual reasoning.
   3. 3 point: I can determine directly from the explanation.
5. **After reading the explanation, do you think the explanation is logically correct?**
   1. 1 point: I think it is inconsistent with the preceding conversation.
   2. 2 point: I think it is consistent with the preceding conversation but logically wrong.
   3. 3 point: I think it is consistent with the preceding conversation and logically right.

6. **After reading the explanation, do you think the explanation is contextual?**
   1. 1 point: I think the explanation is incoherent with the preceding conversation and is out of context.
   2. 2 point: I think the explanation is incoherent with some parts of the preceding conversation, but the whole is still in context.
   3. 3 point: I think the explanation is coherent with the preceding conversation and is in context.
7. **After reading the explanation, do you trust the recommendations given by system?**
   1. 1 point: I don't trust the recommendations at all.
   2. 2 point: I trust the recommendations a bit.
   3. 3 point: I fully trust the recommendations.
8. **After reading the explanation, do you want to try the recommended movies?**
   1. 1 point: I don't want to try at all.
   2. 2 point: I want to try a bit.
   3. 3 point: I especially want to try.
9. **After reading the explanation, are you able to quickly determine whether to accept the recommendations?**
   1. 1 point: I'm hard to determine.
   2. 2 point: I need time to determine.
   3. 3 point: I can determine quickly.

10. **Compared to the lack of explanation, do you think the explanation is satisfactory to you?**
    1. 1 point: I am not satisfied.
    2. 2 point: I am more satisfied.
    3. 3 point: I am very satisfied.

11. **After checking the information about the recommended movie, do you think the explanation is suitable?**
    1. 1 point: I think the explanation is inconsistent with the information of recommended movies.
    2. 2 point: I think the explanation is consistent with the information of recommended movies but too general.
    3. 3 point: I think the explanation is consistent and representative of the recommended movie information.

12. **After checking the information about the recommended movie, do your ratings of the recommended movies change?**
    1. 1 point: A lot of change.
    2. 2 point: A little change.
    3. 3 point: Almost no change.

13. **What is your overall rating of the explanation?**
    1. 1 point: Very bad.
    2. 2 point: Bad.
    3. 3 point: Average.
    4. 4 point: Good.
    5. 5 point: Perfect.
